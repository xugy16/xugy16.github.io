<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Dr. Guangyue Xu(ËÆ∏ÂπøË∑É)</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Dr. Guangyue Xu(ËÆ∏ÂπøË∑É)</name>
              </p>
              <p>I am a final-year Machine Learning(ML) and Natural Language Processing(NLP) Ph.D. student at <a href="https://www.cse.msu.edu/">
                Michigan State University</a>,
                jointly supervised by <a href="https://www.cse.msu.edu/~kordjams/"> Prof. Parisa Kordjamshidi</a>
                and <a href="https://web.eecs.umich.edu/~chaijy/"> Prof. Joyce Y. Chai</a>.
                My research focus on pre-training large vision-language models and enhance their generalization abilities.
              </p>
              <p>
                I received my B.E. in Software Engineering from Jilin University
                and M.S. in Computer Science from Tsinghua University.
                I also interned in MSRA's Web Search and Mining Group previously.
              </p>
              <p>
                <span class="highlight">
                  I will graduate in spring 2024 and I am looking for full-time LLM scientist or engineering jobs!
                </span>
              </p>

              <!-- <p>
                At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://www.matthewtancik.com/nerf">NeRF</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
              </p> -->
              <p style="text-align:center">
                <a href="mailto:xuguang3@msu.edu">Email</a> &nbsp/&nbsp
                <a href="data/xugy_cv.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="">‰∏≠ÊñáÁâàÁÆÄÂéÜ</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?hl=en&user=BVbyVlEAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/xugy16?tab=repositories">Github</a> &nbsp/&nbsp
                <!--  <a >ÂæÆ‰ø°Âè∑Ôºöxgy_cn</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/xugy_no_suit.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/xugy_no_suit.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <!-- <p>Note that *contributed equally</p> -->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
    

    

    <tr onmouseout="nipsd_stop()" onmouseover="nipsd_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='graphcore_image'>
            <video  width=100% height=100% muted autoplay loop>
          <source src="images/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video>
        </div> -->
          <img src='images/graph_arch.jpg' width="160", height="100">
        </div>
        <script type="text/javascript">
          function real3dad_start() {
            document.getElementById('cvprw_image').style.opacity = "1";
          }
    
          function real3dad_stop() {
            document.getElementById('cvprw_image').style.opacity = "0";
          }
          real3dad_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2311.05729.pdf">
          <papertitle>GIPCOL: Graph-Injected Soft Prompting for Compositional Zero-Shot Learning</papertitle>
        </a>
        <br>
        <!-- <a href="https://lioryariv.github.io/">Lior Yariv*</a>, -->
        <strong>Guangyue Xu</strong>,
        Parisa Kordjamshid,
        Joyce Chai
        <br>
        <em>WACV</em>, 2024
        <br>
        <!-- / -->
        <!-- <a href="https://www.youtube.com/watch?v=fThKXZ6uDTk">video</a> -->
        <a href="https://github.com/HLR/GIPCOL">project page</a>
        <!-- / -->
        <!-- <a href="https://www.youtube.com/watch?v=fThKXZ6uDTk">video</a> -->
        /
        <a href="https://arxiv.org/pdf/2311.05729.pdf">arXiv</a>
        <p></p>
        <p>
          We introduce a GNN into soft-prompting design to improve CLIP's compositional ability.
        </p>
      </td>
    </tr>

    <tr onmouseout="graphcore_stop()" onmouseover="graphcore_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='graphcore_image'>
            <video  width=100% height=100% muted autoplay loop>
          <source src="images/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video>
        </div> -->
          <img src='images/metarevision.jpg' width="160", height="100">
        </div>
        <script type="text/javascript">
          function graphcore_start() {
            document.getElementById('graphcore_image').style.opacity = "1";
          }

          function graphcore_stop() {
            document.getElementById('graphcore_image').style.opacity = "0";
          }
          graphcore_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2311.01580.pdf">
          <papertitle>MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition</papertitle>
        </a>
        <br>
        <!-- <a href="https://lioryariv.github.io/">Lior Yariv*</a>, -->
        <strong>Guangyue Xu</strong>,
        Parisa Kordjamshid,
        Joyce Chai
        <br>
        <em>EMNLP-Finding</em>, 2023
        <br>
        <!-- <a href="https://github.com/M-3LAB/open-iad">project page</a> -->
        <!-- / -->
        <!-- <a href="https://www.youtube.com/watch?v=fThKXZ6uDTk">video</a> -->
        <a href="https://github.com/HLR/MetaReVision">project page</a>
        /
        <a href="https://arxiv.org/pdf/2311.01580.pdf">arXiv</a>
        <p></p>
        <p>
          We meta-train vision-language models using retrieved items to obtain more generalizable token representations
          and improve vision-language model's compositional ability.
        </p>
      </td>
    </tr>

    <tr onmouseout="cvprw_stop()" onmouseover="cvprw_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='graphcore_image'>
            <video  width=100% height=100% muted autoplay loop>
          <source src="images/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video>
        </div> -->
          <img src='images/three_prompts_v2.jpg' width="160" height="100">
        </div>
        <script type="text/javascript">
          function iadsurvey_start() {
            document.getElementById('cvprw_image').style.opacity = "1";
          }

          function imiad_stop() {
            document.getElementById('cvprw_image').style.opacity = "0";
          }
          imiad_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2211.05077.pdf">
          <papertitle>Prompting large pre-trained vision-language models for compositional concept learning</papertitle>
        </a>
        <br>
        <strong>Guangyue Xu</strong>,
        Parisa Kordjamshid,
        Joyce Chai
        <br>
        <em><a href="https://arxiv.org/pdf/2211.05077.pdf">arXiv</a></em>, 2022
        <br>
        <!-- / -->
        <!-- <a href="https://www.youtube.com/watch?v=fThKXZ6uDTk">video</a> -->
        <p></p>
        <p>
          We systematically investigate various prompting techniques for CLIP in compositional zero-shot learning.
        </p>
      </td>
    </tr>


    <tr onmouseout="nipsd_stop()" onmouseover="nipsd_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='graphcore_image'>
            <video  width=100% height=100% muted autoplay loop>
          <source src="images/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video>
        </div> -->
          <img src='images/workshop_czsl.jpg' width="160" height="100">
        </div>
        <script type="text/javascript">
          function easynet_start() {
            document.getElementById('mm_image').style.opacity = "1";
          }

          function easynet_stop() {
            document.getElementById('mm_image').style.opacity = "0";
          }
          easynet_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://aclanthology.org/2021.metanlp-1.3.pdf">
          <papertitle>Zero-Shot Compositional Concept Learning</papertitle>
        </a>
        <br>
        <!-- <a href="https://lioryariv.github.io/">Lior Yariv*</a>, -->
        <strong>Guangyue Xu</strong>,
        Parisa Kordjamshid,
        Joyce Chai
        <br>
        <em>MetaNLP@ACL</em>, 2021
        <br>
        <!-- / -->
        <!-- <a href="https://www.youtube.com/watch?v=fThKXZ6uDTk">video</a> -->
        <a href="https://arxiv.org/pdf/2107.05176.pdf">arXiv</a>
        <p></p>
        <p>
          We propose a multi-modality reconstruction-based network for RGBD AD, which eliminate the usage of memory bank and pretrained model. Moreover, the proposed method obtains the best trade-off between the accuracy and inference speed.
        </p>
      </td>
    </tr>



    <tr onmouseout="imiad_stop()" onmouseover="imiad_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='graphcore_image'>
            <video  width=100% height=100% muted autoplay loop>
          <source src="images/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video>
        </div> -->
          <img src='images/lang2action.jpg' width="160">
        </div>
        <script type="text/javascript">
          function imiad_start() {
            document.getElementById('imiad_image').style.opacity = "1";
          }

          function imiad_stop() {
            document.getElementById('imiad_image').style.opacity = "0";
          }
          imiad_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.ijcai.org/proceedings/2018/0001.pdf">
          <papertitle>Language to Action: Towards Interactive Task Learning with Physical Agents</papertitle>
        </a>
        <br>
        <!-- <a href="https://lioryariv.github.io/">Lior Yariv*</a>, -->
        Joyce Chai, Qiaozi Gao, Lanbo She, Shaohua Yang, Sari Saba-Sadiya, <strong>Guangyue Xu</strong>
        <br>
        <em>IJCAI-ECAI (Invited Paper)</em>, 2018
        <br>
        <!-- <a href="https://github.com/M-3LAB/open-iad">project page</a> -->
        <!-- / -->
        <!-- <a href="https://www.youtube.com/watch?v=fThKXZ6uDTk">video</a> -->
        <a href="https://www.ijcai.org/proceedings/2018/0001.pdf">arXiv</a>
        <p></p>
        <p>
          Thoughts and positions about the importance of language communication in human learning and knowledge acquisition.
        </p>
      </td>
    </tr>

    <tr onmouseout="iadsurvey_stop()" onmouseover="iadsurvey_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='graphcore_image'>
            <video  width=100% height=100% muted autoplay loop>
          <source src="images/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video>
        </div> -->
          <img src='images/lda_tracking.jpg' width="160", height="100">
        </div>
        <script type="text/javascript">
          function iadsurvey_start() {
            document.getElementById('iadsurvey_image').style.opacity = "1";
          }

          function imiad_stop() {
            document.getElementById('iadsurvey_image').style.opacity = "0";
          }
          imiad_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://dl.acm.org/doi/pdf/10.1145/3127540.3127567">
          <papertitle>Tracking You Through DNS Traffic: Linking User Sessions By Clustering With Dirichlet Mixture Model</papertitle>
        </a>
        <br>
        <!-- <a href="https://lioryariv.github.io/">Lior Yariv*</a>, -->
        <strong>Guangyue Xu</strong>,
        Mingxuan Sun, Junjie Zhang, Dae Wook Kim
        <br>
        <em>MSWiM</em>, 2017
        <br>
        <!-- / -->
        <!-- <a href="https://www.youtube.com/watch?v=fThKXZ6uDTk">video</a> -->

        <a href="https://dl.acm.org/doi/pdf/10.1145/3127540.3127567">arXiv</a>
        <p></p>
        <p>
          We provide a Bayesian nonparametric model, constrained Dirichlet multinomial mixture (CDMM),
          to analyze user behavior based on the Domain Name System(DNS) data.
        </p>
      </td>
    </tr>
  </tbody></table>



  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td>
            <heading>Service and Activities</heading>
          </td>
        </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/reviewer.jpg" width="160"></td>
          <td width="75%" valign="center">
            <strong>Reviewer</strong>:  EACL 2021, EMNLP 2023, ACM MM 2023, EACL 2023, ACL 2023

            <br><br>
          </td>
        </tr>
      </td>
    </tr>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Teaching</heading>
        <ul>
          <li>CSE 102: Algorithmic Thinking and Programming(Spring 2020, Fall 2020, Fall 2021) - Michigan State University - Teaching Assistant</li>
          <li>CSE 231: Introduction to Programming I(Spring 2022, Fall 2022) - Michigan State University - Teaching Assistant</li>
          <li>CSE 232: Introduction to Programming II(Spring 2023) - Michigan State Univesity - Teaching Assistant</li>
          <li>CSE 440: Introduction to Artificial Intelligence(Fall 2019, Spring 2021) - Michigan State University - Teaching Assistant</li>
        </ul>
      </td>
    </tr>
    </tbody>
  </table>


  <!-------------------------- Stats----------------->
  <table style="width:50%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=HYTrP6QYJH34-ggSCtQjePgBH6qfIvPZgLDi6Sg4B-o&cl=ffffff&w=a"></script>
  </tr>
  </tbody></table>



  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template from <a href="https://jonbarron.info">JonBarron</a>.
              </p>
            </td>
          </tr>
    </tbody>
  </table>
      </td>
    </tr>
  </table>
</body>

</html>
